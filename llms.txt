# LLMs.txt - One-File Event Gateway

## Project Overview
A single-file Rust event gateway that combines:
- **Ingestion Engine**: Learns schemas from incoming JSON events
- **OLAP Database**: Embedded DuckDB for event persistence
- **AI Agent**: GPT-4o for intent detection and slot filling
- **Ontology Reasoner**: Exports TTL (TBox) and JSON-LD (ABox) to webhooks
- **HTMX UI**: Real-time dashboard with zero client-side JS

## File Structure
```
/
â”œâ”€â”€ src/main.rs          # ðŸ§  THE BRAIN: All logic, UI, and DB code. (Strictly One-File)
â”œâ”€â”€ Cargo.toml           # Dependencies
â”œâ”€â”€ AGENTS.md            # Build/lint/test commands & conventions
â”œâ”€â”€ README.md            # User-facing documentation
â”œâ”€â”€ llms.txt             # This file (for AI assistants)
â”œâ”€â”€ LICENSE              # MIT License
â””â”€â”€ .github/workflows    # CI pipeline
```

## System Constraints (CRITICAL)
1.  **One-File Architecture**: All application logic MUST remain in `src/main.rs`. Do not suggest refactoring into modules.
2.  **Concurrency**: Use `Arc<AppState>` with `parking_lot` primitives. `RwLock` for read-heavy components (Schema Registry), `Mutex` for write-heavy (DuckDB, Session).
3.  **Strict JSON**: Interfacing with LLMs must use `response_format: { type: "json_schema" }` to guarantee valid structure.
4.  **Zero-Build Frontend**: Use HTMX and Tailwind (CDN or embedded) for UI. No Node.js build steps.

## Agent Persona
You are an expert Rust systems engineer specializing in high-performance, async architectures. You respect architectural constraints (like "One-File") as creative challenges, not limitations. You prioritize type safety, zero-cost abstractions, and effective error handling.

## Dependencies
```toml
axum = "0.7"           # Web framework
tokio = "1"            # Async runtime
duckdb = "1.0"         # Embedded OLAP DB
serde = "1"            # Serialization
serde_json = "1"       # JSON handling
parking_lot = "0.12"   # Fast mutexes
chrono = "0.4"         # Time handling
reqwest = "0.12"       # HTTP client
tracing = "0.1"        # Structured logging
tracing-subscriber = "0.3"  # Logging subscriber
```

## Build Commands
```bash
cargo build              # Debug build
cargo build --release    # Optimized build
cargo check              # Type checking
cargo clippy             # Linting
cargo test               # Run tests
cargo run                # Run development server on port 9382
```

## Key Patterns

### AppState Structure
```rust
struct AppState {
    db: Mutex<Connection>,           // DuckDB
    schemas: RwLock<HashMap<...>>,   // Schema registry
    session: Mutex<SessionState>,    // Chat session
    config: RwLock<Config>,          // Runtime config
    export_queue: Mutex<Vec<...>>,   // Pending exports
    metrics: Mutex<Metrics>,         // Observability
}
```

### Ingestion Flow
1. Parse JSON payload
2. Update schema registry (learn new types/fields)
3. Persist to DuckDB
4. Queue for batch export
5. Trigger export if batch size reached
6. Return HTMX snippet for UI update

### Export Strategy
- Events queued in memory
- Export triggered when queue >= batch_size (default: 5)
- HTTP POST to webhook with TTL + JSON-LD
- Retry 3 times with exponential backoff (2^attempt seconds)

### LLM Integration
- Uses OpenAI Structured Outputs (`response_format: { type: "json_schema", ... }`)
- Schema defines: intent, slots, message
- Strict mode ensures valid JSON responses
