# LLMs.txt - One-File Event Gateway + Physics Engine

## Project Overview
A single-binary Rust application serving as a high-performance Event Gateway with Zero-Copy ML streaming.

**Stack:**
- **Core**: Rust + Axum + Tokio
- **Database**: DuckDB + Arrow IPC
- **AI (Logic)**: LM Studio (Qwen-Thinking, etc.)
- **AI (Feature Eng)**: Local ONNX (BGE-M3, GLiNER, OAR-OCR) via `ort`
- **Physics Engine**: Shared Memory + SeqLock + Deep Zero-Copy
- **UI**: Ratatui (Terminal)
- **Consumer**: Python + NumPy/PyTorch

## File Structure
```
/
├── src/main.rs          # API, DuckDB, LLM integration
├── src/collider.rs      # Physics Engine: SharedMem, SeqLock, IoBinding
├── src/gliner.rs        # Zero-shot NER Engine
├── src/ocr_pipeline.rs  # Document AI (OAR-OCR)
├── src/tui.rs           # Ratatui terminal UI
├── scripts/consumer.py  # Python zero-copy consumer
├── Cargo.toml           # Dependencies
└── .env                 # LLM_BASE_URL config
```

## System Constraints (CRITICAL)

1. **Zero-Copy Always**: 
   - DuckDB → Arrow RecordBatch → TUI (no struct conversion)
   - Physics Engine (ONNX) → Shared Memory → Python numpy (direct memory access)

2. **Lock-Free Reads**: SeqLock protocol enables concurrent read/write

3. **Thinking Models**: Supports `<think>...</think>` wrapped output (Qwen3, etc.)

4. **Local Execution**: 
   - LLM: LM Studio at `http://192.168.0.141:1234/v1`
   - Embeddings/NER/OCR: Inside the binary via ONNX Runtime

## Dependencies
```toml
axum = "0.7"
tokio = { version = "1", features = ["full"] }
duckdb = { version = "1.0", features = ["bundled", "vtab-arrow"] }
ratatui = "0.29"
reqwest = { version = "0.12", features = ["json", "blocking"] }
memmap2 = "0.9"
ort = { version = "2.0", features = ["load-dynamic"] }
tokenizers = "0.19"
```

## Key Components

### Physics Engine (collider.rs)
- **ParticleFrame**: 4KB struct with 1024-dim embedding, kinetics, spin
- **IoBinding**: Writes inference results directly to `ParticleFrame.semantic`
- **Ring Buffer**: 100,000 frames in shared memory
- **SeqLock**: Odd sequence = writing, Even = valid

### LLM Integration (main.rs)
- **call_llm()**: Posts to LM Studio `/chat/completions` for intent/logic
- **strip_thinking_chain()**: Removes `<think>` tags, extracts JSON
- **LLMResponse**: Optional fields for flexible model support

### Python Consumer (scripts/consumer.py)
- **PhysicsConsumer.follow()**: Generator for real-time frames
- **PyTorchConsumer**: Direct tensor output
- **SeqLock validation**: Retry on torn reads

## Data Flow

```
Event → LLM (intent) → DuckDB → Physics Engine → Shared Memory → Python
```

## Running

```bash
# Gateway (macOS)
DYLD_LIBRARY_PATH="/opt/homebrew/lib" cargo run --release -- --no-tui

# Consumer
python scripts/consumer.py --follow

# Test
curl -X POST http://localhost:9382/ingest -H "Content-Type: application/json" \
  -d '{"message": "Test message", "type": "Test"}'
```
